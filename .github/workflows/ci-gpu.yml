name: CI • Windows Self-Hosted + GPU

on:
  push:
    paths:
      - 'fastapi/**'
      - '.github/workflows/ci-gpu.yml'
  pull_request:
    paths:
      - 'fastapi/**'
      - '.github/workflows/ci-gpu.yml'

permissions:
  contents: read

concurrency:
  group: ci-gpu-${{ github.ref }}
  cancel-in-progress: true

jobs:
  gpu-tests:
    runs-on: [self-hosted, Windows, gpu, cuda, nvidia]
    timeout-minutes: 45
    defaults:
      run:
        shell: powershell
    env:
      PYTHONUTF8: "1"
      HF_HOME: ${{ github.workspace }}\fastapi\models_cache\huggingface
      TORCH_HOME: ${{ github.workspace }}\fastapi\models_cache\torch
      PYTHONPATH: ${{ github.workspace }}\fastapi

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Print runner info
        run: |
          $PSVersionTable.PSVersion
          python --version
          where.exe python || which python

      - name: Show NVIDIA / CUDA
        continue-on-error: true
        run: |
          nvidia-smi
          $env:CUDA_PATH
          $env:CUDA_HOME

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          check-latest: true

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~\AppData\Local\pip\Cache
          key: gpu-${{ runner.os }}-${{ hashFiles('fastapi/requirements.txt', 'fastapi/requirements-dev.txt') }}
          restore-keys: |
            gpu-${{ runner.os }}-

      - name: Ensure model cache folders
        run: |
          New-Item -ItemType Directory -Force -Path "$env:HF_HOME" | Out-Null
          New-Item -ItemType Directory -Force -Path "$env:TORCH_HOME" | Out-Null

      - name: Install deps (FastAPI)
        working-directory: fastapi
        run: |
          python -m pip install --upgrade pip wheel
          pip install -r requirements.txt
          if (Test-Path requirements-dev.txt) { pip install -r requirements-dev.txt } else { pip install pytest }


      - name: Torch / CUDA sanity check
        working-directory: fastapi
        run: |
          python - << 'PY'
          import torch, platform
          print("Torch:", torch.__version__)
          print("CUDA available:", torch.cuda.is_available())
          if torch.cuda.is_available():
              print("CUDA devices:", torch.cuda.device_count())
              print("Current device:", torch.cuda.current_device())
              print("Device name:", torch.cuda.get_device_name(0))
          else:
              raise SystemExit("CUDA not available on this runner.")
          PY

      - name: Run GPU tests
        working-directory: fastapi
        run: |
          # إن كنت تستخدم ماركر gpu في pytest:
          # python -m pytest -m "gpu" -q
          # أو نفّذ كل الاختبارات بما فيها GPU:
          python -m pytest -q

      
      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: gpu-ci-logs
          path: |
            fastapi/.pytest_cache
            fastapi/logs
            fastapi/models_cache
